#!/usr/bin/env python3
"""
å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä¸€æ‹¬å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

èªè¨¼æƒ…å ±ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ASP/ãƒ¡ãƒ‡ã‚£ã‚¢ã®çµ„ã¿åˆã‚ã›ã«å¯¾ã—ã¦ã€
åˆ©ç”¨å¯èƒ½ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ã™ã¹ã¦å®Ÿè¡Œã™ã‚‹ã€‚

Usage:
    # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã™ã¹ã¦ï¼‰
    python run_all_scrapers.py --daily

    # æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã™ã¹ã¦ï¼‰
    python run_all_scrapers.py --monthly

    # æ—¥æ¬¡ã¨æœˆæ¬¡ã®ä¸¡æ–¹ã‚’å–å¾—
    python run_all_scrapers.py --daily --monthly

    # ç‰¹å®šã®ASPã ã‘å®Ÿè¡Œ
    python run_all_scrapers.py --daily --asp moshimo

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿè¡Œã›ãšã«å¯¾è±¡ã‚’ç¢ºèªï¼‰
    python run_all_scrapers.py --daily --dry-run

    # ãƒ–ãƒ©ã‚¦ã‚¶è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
    python run_all_scrapers.py --daily --no-headless
"""
import os
import sys
import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from dotenv import load_dotenv
load_dotenv(Path(__file__).resolve().parent.parent / '.env')

from supabase import create_client


# åˆ©ç”¨å¯èƒ½ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã¨ASPåã®ãƒãƒƒãƒ”ãƒ³ã‚°
# ã™ã¹ã¦æ–°å½¢å¼ï¼ˆBaseScraperä½¿ç”¨ã€asp_credentialsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰èªè¨¼æƒ…å ±å–å¾—ï¼‰
AVAILABLE_SCRAPERS = {
    'moshimo': {
        'module': 'scrapers.moshimo.scraper',
        'class_name': 'MoshimoScraper',
        'asp_patterns': ['ã‚‚ã—ã‚‚', 'moshimo'],
        'supports_daily': True,
        'supports_monthly': True,
    },
    'accesstrade': {
        'module': 'scrapers.accesstrade.scraper',
        'class_name': 'AccesstradeScraper',
        'asp_patterns': ['ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¬ãƒ¼ãƒ‰', 'accesstrade'],
        'supports_daily': True,
        'supports_monthly': True,
    },
    'ultelo': {
        'module': 'scrapers.ultelo.scraper',
        'class_name': 'UlteloScraper',
        'asp_patterns': ['ultelo', 'ã‚¢ãƒ«ãƒ†ã‚¬'],
        'supports_daily': True,
        'supports_monthly': True,
    },
    'valuecommerce': {
        'module': 'scrapers.valuecommerce.scraper',
        'class_name': 'ValueCommerceScraper',
        'asp_patterns': ['valuecommerce', 'ãƒãƒªãƒ¥ãƒ¼ã‚³ãƒãƒ¼ã‚¹'],
        'supports_daily': False,  # æ—¥æ¬¡ã¯ã¾ã æœªå®Ÿè£…
        'supports_monthly': True,
    },
    'a8net': {
        'module': 'scrapers.a8net.scraper',
        'class_name': 'A8netScraper',
        'asp_patterns': ['a8.net', 'a8ï¼ˆ'],
        'supports_daily': True,
        'supports_monthly': True,
    },
    'a8app': {
        'module': 'scrapers.a8app.scraper',
        'class_name': 'A8appScraper',
        'asp_patterns': ['seedapp', 'a8app'],
        'supports_daily': True,
        'supports_monthly': False,
    },
    'afb': {
        'module': 'scrapers.afb.scraper',
        'class_name': 'AfbScraper',
        'asp_patterns': ['afb', 'ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆb'],
        'supports_daily': True,
        'supports_monthly': False,
    },
    'affitown': {
        'module': 'scrapers.affitown.scraper',
        'class_name': 'AffitownScraper',
        'asp_patterns': ['affitown', 'ã‚¢ãƒ•ã‚£ã‚¿ã‚¦ãƒ³'],
        'supports_daily': True,
        'supports_monthly': True,
    },
    'circuitx': {
        'module': 'scrapers.circuitx.scraper',
        'class_name': 'CircuitxScraper',
        'asp_patterns': ['circuitx', 'ã‚µãƒ¼ã‚­ãƒƒãƒˆx'],
        'supports_daily': True,
        'supports_monthly': False,
    },
    'felmat': {
        'module': 'scrapers.felmat.scraper',
        'class_name': 'FelmatScraper',
        'asp_patterns': ['felmat', 'ãƒ•ã‚§ãƒ«ãƒ'],
        'supports_daily': True,
        'supports_monthly': True,
    },
    'gmosmaaffi': {
        'module': 'scrapers.gmosmaaffi.scraper',
        'class_name': 'GmoSmaaffiScraper',
        'asp_patterns': ['smaad', 'gmo', 'ã‚¹ãƒãƒ¼ãƒˆã‚¢ãƒ•ã‚£ãƒª'],
        'supports_daily': True,
        'supports_monthly': True,
    },
    'linkag': {
        'module': 'scrapers.linkag.scraper',
        'class_name': 'LinkagScraper',
        'asp_patterns': ['link-ag', 'linkag', 'ãƒªãƒ³ã‚¯ã‚¨ãƒ¼ã‚¸'],
        'supports_daily': True,
        'supports_monthly': False,
    },
    'webridge': {
        'module': 'scrapers.webridge.scraper',
        'class_name': 'WebridgeScraper',
        'asp_patterns': ['webridge', 'ã‚¦ã‚§ãƒ–ãƒªãƒƒã‚¸'],
        'supports_daily': True,
        'supports_monthly': True,
    },
}


def get_supabase():
    """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
    return create_client(
        os.getenv('SUPABASE_URL'),
        os.getenv('SUPABASE_SERVICE_ROLE_KEY')
    )


def get_scraper_for_asp(asp_name: str) -> Optional[dict]:
    """ASPåã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼æƒ…å ±ã‚’å–å¾—"""
    asp_lower = asp_name.lower()
    for scraper_key, scraper_info in AVAILABLE_SCRAPERS.items():
        for pattern in scraper_info['asp_patterns']:
            if pattern.lower() in asp_lower:
                return {**scraper_info, 'key': scraper_key}
    return None


def load_scraper_class(scraper_info: dict):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã‚’å‹•çš„ã«ãƒ­ãƒ¼ãƒ‰"""
    module = __import__(scraper_info['module'], fromlist=[''])
    return getattr(module, scraper_info['class_name'])


def get_all_credentials_with_asps():
    """èªè¨¼æƒ…å ±ã¨ASPæƒ…å ±ã‚’å–å¾—"""
    supabase = get_supabase()

    result = supabase.table('asp_credentials').select(
        'id, asp_id, media_id, asps(id, name, is_active), media(id, name)'
    ).execute()

    return result.data or []


def run_scraper(
    scraper_info: dict,
    asp_id: str,
    media_id: str,
    asp_name: str,
    media_name: str,
    run_daily: bool,
    run_monthly: bool,
    headless: bool,
    max_retries: int
) -> dict:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’å®Ÿè¡Œ"""
    results = {
        'asp_name': asp_name,
        'media_name': media_name,
        'scraper': scraper_info['key'],
        'daily': None,
        'monthly': None,
    }

    try:
        ScraperClass = load_scraper_class(scraper_info)

        # ValueCommerceã¯ç‰¹æ®Šãªã®ã§åˆ¥å‡¦ç†
        if scraper_info['key'] == 'valuecommerce':
            scraper = ScraperClass(asp_id=asp_id, media_id=media_id)
        else:
            scraper = ScraperClass(
                asp_id=asp_id,
                media_id=media_id,
                headless=headless,
                max_retries=max_retries
            )

        # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿å–å¾—
        if run_daily and scraper_info['supports_daily']:
            print(f"    ğŸ“Š æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            try:
                result = scraper.run_daily()
                results['daily'] = result
                if result.get('success'):
                    print(f"    âœ… æ—¥æ¬¡: æˆåŠŸ ({result.get('records_count', 0)}ä»¶)")
                else:
                    print(f"    âŒ æ—¥æ¬¡: å¤±æ•— - {result.get('error', 'Unknown error')}")
            except Exception as e:
                results['daily'] = {'success': False, 'error': str(e)}
                print(f"    âŒ æ—¥æ¬¡: ã‚¨ãƒ©ãƒ¼ - {e}")

        # æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿å–å¾—
        if run_monthly and scraper_info['supports_monthly']:
            print(f"    ğŸ“ˆ æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            try:
                result = scraper.run_monthly()
                results['monthly'] = result
                if result.get('success'):
                    print(f"    âœ… æœˆæ¬¡: æˆåŠŸ ({result.get('records_count', 0)}ä»¶)")
                else:
                    print(f"    âŒ æœˆæ¬¡: å¤±æ•— - {result.get('error', 'Unknown error')}")
            except Exception as e:
                results['monthly'] = {'success': False, 'error': str(e)}
                print(f"    âŒ æœˆæ¬¡: ã‚¨ãƒ©ãƒ¼ - {e}")

    except Exception as e:
        print(f"    âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        results['error'] = str(e)

    return results


def main():
    parser = argparse.ArgumentParser(
        description="å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä¸€æ‹¬å®Ÿè¡Œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã™ã¹ã¦ã®ASPï¼‰
  python run_all_scrapers.py --daily

  # æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã™ã¹ã¦ã®ASPï¼‰
  python run_all_scrapers.py --monthly

  # ä¸¡æ–¹å–å¾—
  python run_all_scrapers.py --daily --monthly

  # ç‰¹å®šã®ASPã ã‘å®Ÿè¡Œï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
  python run_all_scrapers.py --daily --asp moshimo

  # å®Ÿè¡Œå¯¾è±¡ã‚’ç¢ºèªï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼‰
  python run_all_scrapers.py --daily --dry-run

  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªASPã®ã¿å®Ÿè¡Œ
  python run_all_scrapers.py --daily --active-only
        """
    )

    # å®Ÿè¡Œã‚¿ã‚¤ãƒ—
    parser.add_argument('--daily', action='store_true', help='æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—')
    parser.add_argument('--monthly', action='store_true', help='æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—')

    # ãƒ•ã‚£ãƒ«ã‚¿
    parser.add_argument('--asp', help='ç‰¹å®šã®ASPã®ã¿å®Ÿè¡Œï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰')
    parser.add_argument('--media', help='ç‰¹å®šã®ãƒ¡ãƒ‡ã‚£ã‚¢ã®ã¿å®Ÿè¡Œï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰')
    parser.add_argument('--active-only', action='store_true', help='is_active=Trueã®ASPã®ã¿å®Ÿè¡Œ')

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--no-headless', action='store_true', help='ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤º')
    parser.add_argument('--retries', type=int, default=3, help='ãƒªãƒˆãƒ©ã‚¤å›æ•°')
    parser.add_argument('--dry-run', action='store_true', help='å®Ÿè¡Œã›ãšã«å¯¾è±¡ã‚’ç¢ºèª')
    parser.add_argument('--output', help='çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜')

    args = parser.parse_args()

    # å¿…é ˆãƒã‚§ãƒƒã‚¯
    if not args.daily and not args.monthly:
        parser.print_help()
        print("\nError: --daily ã¾ãŸã¯ --monthly ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        sys.exit(1)

    print("=" * 60)
    print("ğŸš€ å…¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä¸€æ‹¬å®Ÿè¡Œ")
    print(f"   é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   æ—¥æ¬¡: {'Yes' if args.daily else 'No'}")
    print(f"   æœˆæ¬¡: {'Yes' if args.monthly else 'No'}")
    if args.asp:
        print(f"   ASPãƒ•ã‚£ãƒ«ã‚¿: {args.asp}")
    if args.media:
        print(f"   ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚£ãƒ«ã‚¿: {args.media}")
    if args.active_only:
        print(f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã®ã¿: Yes")
    print("=" * 60)

    # èªè¨¼æƒ…å ±å–å¾—
    credentials = get_all_credentials_with_asps()
    print(f"\nğŸ“‹ èªè¨¼æƒ…å ±: {len(credentials)}ä»¶")

    # å®Ÿè¡Œå¯¾è±¡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    targets = []
    for cred in credentials:
        asp_info = cred.get('asps', {})
        media_info = cred.get('media', {})
        asp_name = asp_info.get('name', 'Unknown')
        media_name = media_info.get('name', 'Unknown')
        is_active = asp_info.get('is_active', False)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒã‚§ãƒƒã‚¯
        if args.asp and args.asp.lower() not in asp_name.lower():
            continue
        if args.media and args.media.lower() not in media_name.lower():
            continue
        if args.active_only and not is_active:
            continue

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        scraper_info = get_scraper_for_asp(asp_name)
        if not scraper_info:
            continue

        # æ—¥æ¬¡/æœˆæ¬¡ã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        if args.daily and not scraper_info['supports_daily']:
            if not args.monthly:
                continue
        if args.monthly and not scraper_info['supports_monthly']:
            if not args.daily:
                continue

        targets.append({
            'cred': cred,
            'asp_name': asp_name,
            'media_name': media_name,
            'asp_id': cred['asp_id'],
            'media_id': cred['media_id'],
            'scraper_info': scraper_info,
            'is_active': is_active,
        })

    print(f"\nğŸ¯ å®Ÿè¡Œå¯¾è±¡: {len(targets)}ä»¶")
    for i, target in enumerate(targets, 1):
        status = "ğŸŸ¢" if target['is_active'] else "âšª"
        daily_support = "D" if target['scraper_info']['supports_daily'] else "-"
        monthly_support = "M" if target['scraper_info']['supports_monthly'] else "-"
        print(f"   {i}. {status} {target['asp_name']} / {target['media_name']} [{daily_support}{monthly_support}]")

    print("\n   å‡¡ä¾‹: ğŸŸ¢=ç¨¼åƒä¸­, âšª=æœªç¨¼åƒ, D=æ—¥æ¬¡å¯¾å¿œ, M=æœˆæ¬¡å¯¾å¿œ")

    if not targets:
        print("\nâš ï¸  å®Ÿè¡Œå¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“")
        sys.exit(0)

    if args.dry_run:
        print("\nğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³çµ‚äº†ï¼ˆå®Ÿéš›ã®å®Ÿè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        sys.exit(0)

    # å®Ÿè¡Œ
    print("\n" + "=" * 60)
    print("ğŸ“¥ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
    print("=" * 60)

    all_results = []
    success_count = 0
    fail_count = 0

    for i, target in enumerate(targets, 1):
        print(f"\n[{i}/{len(targets)}] {target['asp_name']} / {target['media_name']}")

        result = run_scraper(
            scraper_info=target['scraper_info'],
            asp_id=target['asp_id'],
            media_id=target['media_id'],
            asp_name=target['asp_name'],
            media_name=target['media_name'],
            run_daily=args.daily,
            run_monthly=args.monthly,
            headless=not args.no_headless,
            max_retries=args.retries,
        )

        all_results.append(result)

        # æˆåŠŸ/å¤±æ•—ã‚«ã‚¦ãƒ³ãƒˆ
        daily_result = result.get('daily') or {}
        monthly_result = result.get('monthly') or {}
        if daily_result.get('success') or monthly_result.get('success'):
            success_count += 1
        else:
            fail_count += 1

    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ“Š å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    print(f"   çµ‚äº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   æˆåŠŸ: {success_count}ä»¶")
    print(f"   å¤±æ•—: {fail_count}ä»¶")

    # è©³ç´°çµæœ
    print("\nè©³ç´°:")
    for result in all_results:
        daily_r = result.get('daily') or {}
        monthly_r = result.get('monthly') or {}
        status = "âœ…" if daily_r.get('success') or monthly_r.get('success') else "âŒ"
        daily_count = daily_r.get('records_count', '-') if daily_r else '-'
        monthly_count = monthly_r.get('records_count', '-') if monthly_r else '-'
        print(f"   {status} {result['asp_name']} / {result['media_name']}: æ—¥æ¬¡={daily_count}, æœˆæ¬¡={monthly_count}")

    # JSONå‡ºåŠ›
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total': len(targets),
                    'success': success_count,
                    'fail': fail_count,
                },
                'results': all_results,
            }, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜: {args.output}")

    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    sys.exit(0 if fail_count == 0 else 1)


if __name__ == "__main__":
    main()
